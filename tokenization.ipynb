{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b979e5-65e2-4b1b-a96f-4ecb1fb98db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b9e2403-de07-4f04-99a9-e98850d61039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Welcome to GeeksforGeeks.',\n",
       " 'You are studying NLP article.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
    "a=sent_tokenize(text, language='english')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "885a1a26-9ae5-44df-b478-28fcec61485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d25483c-64b3-42ee-b5af-9a168bc010b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.']\n",
      "['Welcome to GeeksforGeeks.']\n",
      "['You are studying NLP article.']\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(sent_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08393776-b067-4de6-9a06-d5ec4773287a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'botany',\n",
       " ',',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'is',\n",
       " 'a',\n",
       " 'perennial',\n",
       " 'plant',\n",
       " 'with',\n",
       " 'an',\n",
       " 'elongated',\n",
       " 'stem',\n",
       " ',',\n",
       " 'or',\n",
       " 'trunk',\n",
       " ',',\n",
       " 'usually',\n",
       " 'supporting',\n",
       " 'branches',\n",
       " 'and',\n",
       " 'leaves',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'usages',\n",
       " ',',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'may',\n",
       " 'be',\n",
       " 'narrower',\n",
       " ',',\n",
       " 'including',\n",
       " 'only',\n",
       " 'woody',\n",
       " 'plants',\n",
       " 'with',\n",
       " 'secondary',\n",
       " 'growth',\n",
       " ',',\n",
       " 'plants',\n",
       " 'that',\n",
       " 'are',\n",
       " 'usable',\n",
       " 'as',\n",
       " 'lumber',\n",
       " 'or',\n",
       " 'plants',\n",
       " 'above',\n",
       " 'a',\n",
       " 'specified',\n",
       " 'height',\n",
       " '.',\n",
       " 'In',\n",
       " 'wider',\n",
       " 'definitions',\n",
       " ',',\n",
       " 'the',\n",
       " 'taller',\n",
       " 'palms',\n",
       " ',',\n",
       " 'tree',\n",
       " 'ferns',\n",
       " ',',\n",
       " 'bananas',\n",
       " ',',\n",
       " 'and',\n",
       " 'bamboos',\n",
       " 'are',\n",
       " 'also',\n",
       " 'trees',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"In botany, a tree is a perennial plant with an elongated stem, or trunk, usually supporting branches and leaves. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. In wider definitions, the taller palms, tree ferns, bananas, and bamboos are also trees.\"\n",
    "b=word_tokenize(text)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2aef7f02-c055-42c7-a514-665219c5721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "botany\n",
      ",\n",
      "a\n",
      "tree\n",
      "is\n",
      "a\n",
      "perennial\n",
      "plant\n",
      "with\n",
      "an\n",
      "elongated\n",
      "stem\n",
      ",\n",
      "or\n",
      "trunk\n",
      ",\n",
      "usually\n",
      "supporting\n",
      "branches\n",
      "and\n",
      "leaves\n",
      ".\n",
      "In\n",
      "some\n",
      "usages\n",
      ",\n",
      "the\n",
      "definition\n",
      "of\n",
      "a\n",
      "tree\n",
      "may\n",
      "be\n",
      "narrower\n",
      ",\n",
      "including\n",
      "only\n",
      "woody\n",
      "plants\n",
      "with\n",
      "secondary\n",
      "growth\n",
      ",\n",
      "plants\n",
      "that\n",
      "are\n",
      "usable\n",
      "as\n",
      "lumber\n",
      "or\n",
      "plants\n",
      "above\n",
      "a\n",
      "specified\n",
      "height\n",
      ".\n",
      "In\n",
      "wider\n",
      "definitions\n",
      ",\n",
      "the\n",
      "taller\n",
      "palms\n",
      ",\n",
      "tree\n",
      "ferns\n",
      ",\n",
      "bananas\n",
      ",\n",
      "and\n",
      "bamboos\n",
      "are\n",
      "also\n",
      "trees\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0490a5fc-a0cb-4c23-bf36-0e0dcc5b5381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', 'Welcome', 'to', 'GeeksforGeeks']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
    "c=word_tokenize(text)\n",
    "[i for i in c if i.isalpha()]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c0e92d4-e9f6-4491-a7d5-1c03c793ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello']\n",
      "['everyone']\n",
      "['.']\n",
      "['Welcome']\n",
      "['to']\n",
      "['GeeksforGeeks']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52440c04-a051-4539-bb35-0610dafdd498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd955fad-e5e6-4e49-a161-e8eac47da09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Let's see how it's working.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01f01abc-f91e-4091-a36e-0cf14a9c7b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'see', 'how', 'it', 's', 'working']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = \"Let's see how it's working.\"\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcd1df-690b-4437-b7c6-03cc8ba75903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
