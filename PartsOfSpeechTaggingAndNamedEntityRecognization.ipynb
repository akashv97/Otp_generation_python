{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4ebf6d-5413-4c53-a860-b6765507d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCC coordinating conjunction \\nCD cardinal digit \\nDT determiner \\nEX existential there (like: “there is” … think of it like “there exists”) \\nFW foreign word \\nIN preposition/subordinating conjunction \\nJJ adjective – ‘big’ \\nJJR adjective, comparative – ‘bigger’ \\nJJS adjective, superlative – ‘biggest’ \\nLS list marker 1) \\nMD modal – could, will \\nNN noun, singular ‘- desk’ \\nNNS noun plural – ‘desks’ \\nNNP proper noun, singular – ‘Harrison’ \\nNNPS proper noun, plural – ‘Americans’ \\nPDT predeterminer – ‘all the kids’ \\nPOS possessive ending parent’s \\nPRP personal pronoun –  I, he, she \\nPRP$ possessive pronoun – my, his, hers \\nRB adverb – very, silently, \\nRBR adverb, comparative – better \\nRBS adverb, superlative – best \\nRP particle – give up \\nTO – to go ‘to’ the store. \\nUH interjection – errrrrrrrm \\nVB verb, base form – take \\nVBD verb, past tense – took \\nVBG verb, gerund/present participle – taking \\nVBN verb, past participle – taken \\nVBP verb, sing. present, non-3d – take \\nVBZ verb, 3rd person sing. present – takes \\nWDT wh-determiner – which \\nWP wh-pronoun – who, what \\nWP$ possessive wh-pronoun, eg- whose \\nWRB wh-adverb, eg- where, when\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-adverb, eg- where, when\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b523a4b7-50e9-4d01-9a3b-7ed2bc5b0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95aa0fd4-401c-46a1-981a-9b914e206c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8dfcf80-8641-4024-8bae-dd972bef048f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5460498-80ff-44a8-a35a-d21867788471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
      "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Why', 'WRB'), ('?', '.')]\n",
      "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
      "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71e6c9d7-7fc8-4935-b937-103309e1474d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('Monument', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()) ## input we shouldgive in form of list to pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b2f3758-bcb1-438c-9c99-49ec28b44cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'Monument']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Taj Mahal is a beautiful Monument\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ed5021-bd63-42f8-9c57-144b954a52f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerson Eg: Krish C Naik\\nPlace Or Location Eg: India\\nDate Eg: September,24-09-1989\\nTime  Eg: 4:30pm\\nMoney Eg: 1 million dollar\\nOrganization Eg: iNeuron Private Limited\\nPercent Eg: 20%, twenty percent\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "\"\"\"\n",
    "Person Eg: Krish C Naik\n",
    "Place Or Location Eg: India\n",
    "Date Eg: September,24-09-1989\n",
    "Time  Eg: 4:30pm\n",
    "Money Eg: 1 million dollar\n",
    "Organization Eg: iNeuron Private Limited\n",
    "Percent Eg: 20%, twenty percent\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc0e12fd-e387-43bd-81ae-0be03fd04f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1b65c49-d800-4db1-841e-84137f28950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)\n",
    "tag_elements=nltk.pos_tag(words)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c422072-2a0a-4684-9313-ebbec52c3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7617440-1c0f-4183-a7be-760be370705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from click->nltk==3.8.1) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 1.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk==3.8.1\n",
    "!wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
    "!unzip stanford-ner-2015-04-20.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37a1a277-9b95-4d03-b531-8f1bad2d41cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at stanford-ner-2015-04-20/stanford-ner-3.5.2.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m jar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstanford-ner-2015-04-20/stanford-ner-3.5.2.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstanford-ner-2015-04-20/classifiers/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m st_4class \u001b[38;5;241m=\u001b[39m StanfordNERTagger(model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish.conll.4class.distsim.crf.ser.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, jar, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\stanford.py:200\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    199\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m    201\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(tagged_sentences[start:start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(sent)])\n\u001b[0;32m    202\u001b[0m     start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sent)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\stanford.py:70\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmd\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    A property that returns the command that will be executed.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\internals.py:833\u001b[0m, in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ElementTree\u001b[38;5;241m.\u001b[39mtostring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_etree, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m##////////////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m#{ Element interface Delegation (pass-through)\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m##////////////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m--> 833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrib):\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_etree, attrib)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, value):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\internals.py:719\u001b[0m, in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    715\u001b[0m         div \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m75\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (div, msg, div))\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_jar\u001b[39m(name_pattern, path_to_jar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, env_vars\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m--> 719\u001b[0m         searchpath\u001b[38;5;241m=\u001b[39m(), url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m    721\u001b[0m                          searchpath, url, verbose, is_regex))\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_jars_within_path\u001b[39m(path_to_jars):\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at stanford-ner-2015-04-20/stanford-ner-3.5.2.jar"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "jar = \"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\n",
    "model = \"stanford-ner-2015-04-20/classifiers/\"\n",
    "st_4class = StanfordNERTagger(model + \"english.conll.4class.distsim.crf.ser.gz\", jar, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8db6d55-b29d-4ab0-b26b-14a667002e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = '''Deepak Jasani, Head of retail research, HDFC Securities, said: “Investors will look to the European Central Bank later Thursday for reassurance that surging prices are just transitory, and not about to spiral out of control. In addition to the ECB policy meeting, investors are awaiting a report later Thursday on US economic growth, which is likely to show a cooling recovery, as well as weekly jobs data.”.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d6c929e-55b6-40c2-8146-146139647c17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st_4class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m st_4class\u001b[38;5;241m.\u001b[39mtag(example_document\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'st_4class' is not defined"
     ]
    }
   ],
   "source": [
    "st_4class.tag(example_document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85864c99-1d20-46c9-a3c4-31d5ab227116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ranjan\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.7.6-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.1 MB 2.2 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.3/12.1 MB 2.6 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.6/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/12.1 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.6/12.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.8/12.1 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.8/12.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 13.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.6/12.1 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.1 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.7/12.1 MB 14.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.0/12.1 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 12.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.7/12.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.6/12.1 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.0/182.0 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 479.7/479.7 kB 29.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.3/1.5 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 23.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/6.6 MB 43.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 34.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 31.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 22.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.1/6.6 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.8/6.6 MB 20.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 20.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.9/5.4 MB 20.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.4 MB 19.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 19.1 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3279ded-6bcb-427f-9276-116d646becc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy.cli \n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cbb663d7-2782-41e2-a854-158e976a1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c4897af2-3f27-4906-9a8e-0cfec07b2739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Deepak Jasani', 'PERSON'),\n",
       " ('ECB', 'ORG'),\n",
       " ('HDFC Securities', 'ORG'),\n",
       " ('US', 'GPE'),\n",
       " ('later Thursday', 'DATE'),\n",
       " ('the European Central Bank', 'ORG'),\n",
       " ('weekly', 'DATE')}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_large_ner(document):\n",
    "  return {(ent.text.strip(), ent.label_) for ent in sp_sm(document).ents}\n",
    "spacy_large_ner(example_document)\n",
    "#gpe=geopolitical entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3e2a1-90be-4cd2-8b14-b47e746a18fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
